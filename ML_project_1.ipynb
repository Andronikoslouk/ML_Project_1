{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Base functions"
      ],
      "metadata": {
        "id": "ZTuboUA0bKt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLGVFUWXEn_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Notice that in this section, we have more parameters than needed, with the unneeded parameters pre-set to 0.\n",
        "#This is because we define a general function for the cross-validation.\n",
        "\n",
        "def mean_square_error_gd(y,tx,initial_w,max_iters,gamma,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  e=y-tx @ initial_w\n",
        "  for i in range(max_iters):\n",
        "    e=y-tx @ w\n",
        "    grad=- (tx.T @ e.copy()) / N\n",
        "    w-=gamma*grad\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def mean_square_error_sgd(y,tx,initial_w,max_iters,gamma,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via stochastic gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  coefs=np.random.choice(N,max_iters)\n",
        "  e=y-tx @ initial_w\n",
        "  for i in range(max_iters):\n",
        "    j=coefs[i]\n",
        "    e=y-tx @ w\n",
        "    stoch_grad=- (tx[j,:].T*e[j])\n",
        "    w-=gamma*stoch_grad\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def least_squares(y,tx,initial_w=0,max_iters=0,gamma=0,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via normal equations of least squares problem.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def ridge_regression(y,tx,lambda_,initial_w=0,max_iters=0,gamma=0):\n",
        "  '''Computes the argmin/min of the L2-regularized quadratic function via normal equations.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   lambda_ is a real number representing the regularization coefficient.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  M=tx.shape[1]\n",
        "  w=np.linalg.solve(tx.T @ tx + 2*N*lambda_*np.eye(M), tx.T @ y)\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def sigmoid(x):\n",
        "  '''Computes the sigmoid function.\n",
        "\n",
        "  '''\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def logistic_regression(y, tx, initial_w, max_iters, gamma, lambda_=0):\n",
        "  '''Computes the argmin/min of the logistic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  for i in range(max_iters):\n",
        "    grad=0\n",
        "    for j in range(N):\n",
        "      grad+=(sigmoid(tx[j,:].T @ w)-y[j])*tx[j,:]\n",
        "    grad/=N\n",
        "    w-=gamma*grad\n",
        "  loss=0\n",
        "  for j in range(N):\n",
        "    w_pert=tx[j,:] @ w\n",
        "    loss+=-y[j]*(w_pert)+np.log(1+np.exp(w_pert))\n",
        "  loss/=N\n",
        "  return w,loss\n",
        "\n",
        "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
        "  '''Computes the argmin/min of the L2-regularized logistic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   lambda_ is a real number representing the regularization coefficient.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  for i in range(max_iters):\n",
        "    grad=0\n",
        "    for j in range(N):\n",
        "      grad+=(sigmoid(tx[j,:].T @ w)-y[j])*tx[j,:]\n",
        "    grad/=N\n",
        "    grad+=2*lambda_*w\n",
        "    w-=gamma*grad\n",
        "  loss=0\n",
        "  for j in range(N):\n",
        "    w_pert=tx[j,:] @ w\n",
        "    loss+=-y[j]*(w_pert)+np.log(1+np.exp(w_pert))\n",
        "  loss/=N\n",
        "  return w,loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation function"
      ],
      "metadata": {
        "id": "f_e46ueSogSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def square_label(tx,w):\n",
        "  '''Returns the outputs based on the quadratic model.\n",
        "  tx is an (N,M) array representing the matrix of inputs.\n",
        "  w is an (M,) array representing the minimum of the quadratic function.\n",
        "\n",
        "  '''\n",
        "  return tx @ w\n",
        "\n",
        "def logistic_label(tx,w):\n",
        "  '''Returns the outputs based on the logistic model.\n",
        "  tx is an (N,M) array representing the matrix of inputs.\n",
        "  w is an (M,) array representing the minimum of the quadratic function.\n",
        "\n",
        "  '''\n",
        "  return np.round(sigmoid(tx @ w))\n",
        "\n",
        "def train_score_square(y_test, y_ML):\n",
        "  '''Computes the L2,L1 scores of the quadratic model.\n",
        "  y_test is an (N,) Boolean vector representing the true labels.\n",
        "  y_ML is an (N,) Boolean vector representing the predicted labels.\n",
        "\n",
        "  '''\n",
        "  N=y_test.shape[0]\n",
        "  return np.mean((y_test-y_ML)**2),np.mean(np.abs(y_test-y_ML))\n",
        "\n",
        "def train_score_logistic(y_test, y_ML):\n",
        "  '''Computes the probabilistic score and the F1 score of the logistic model.\n",
        "  y_test is an (N,) Boolean vector representing the true labels.\n",
        "  y_ML is an (N,) Boolean vector representing the predicted labels.\n",
        "\n",
        "  '''\n",
        "  N=y_test.shape[0]\n",
        "  P=len(np.argwhere(np.logical_and(y_ML==1,y_test==1)))/len(np.argwhere(y_test==1))\n",
        "  R=len(np.argwhere(np.logical_and(y_ML==1,y_test==1)))/len(np.argwhere(y_ML==1))\n",
        "  return np.mean(np.abs(y_test-y_ML)),2*P*R/(P+R)\n",
        "\n",
        "def cross_val(mach_learn,data_repr,score_func,tx,y,folds,max_iters=0,gamma=0,lambda_=0):\n",
        "  '''Cross-validation of the model. Returns two different model scores.\n",
        "  mach_learn - function representing the utilised method of machine learning.\n",
        "  data_repr - function representing the data.\n",
        "  score_func - function representing the score.\n",
        "  tx - (N,M) array representing the matrix of N input vectors.\n",
        "  y - (N,) array representing the vector of N outputs.\n",
        "  folds - integer representing the number of folds.\n",
        "  max_iters - integer representing the training time on each fold.\n",
        "  gamma is a real number representing the step-size.\n",
        "\n",
        "  '''\n",
        "  N,M=tx.shape\n",
        "  fold_size=N//folds\n",
        "  sf=np.random.permutation(N)\n",
        "  scores=np.ones(folds)\n",
        "  alt_scores=np.ones(folds)\n",
        "  for k in range(folds):\n",
        "    train_data=tx[np.append(sf[:k*fold_size],sf[(k+1)*fold_size:])].copy()\n",
        "    train_labels=y[np.append(sf[:k*fold_size],sf[(k+1)*fold_size:])].copy()\n",
        "    test_data=tx[sf[k*fold_size:(k+1)*fold_size]].copy()\n",
        "    test_labels=y[sf[k*fold_size:(k+1)*fold_size]].copy()\n",
        "    w=mach_learn(y=train_labels,tx=train_data,initial_w=np.zeros(M),max_iters=max_iters,gamma=gamma,lambda_=lambda_)[0]\n",
        "    pred_labels=data_repr(test_data,w)\n",
        "    scores[k],alt_scores[k]=score_func(test_labels,pred_labels)\n",
        "  return np.mean(scores),np.mean(alt_scores)"
      ],
      "metadata": {
        "id": "CNVKeqRafgQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "k50ogs6C16JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers import load_csv_data\n",
        "\n",
        "x_train, x_test, y_train,_,_= load_csv_data('/content/')"
      ],
      "metadata": {
        "id": "rvB5Y1y-15Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the medically insignificant data (e.g. - telephone number)"
      ],
      "metadata": {
        "id": "1hyvwnJW2bgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train1=x_train.copy()\n",
        "x_test1=x_test.copy()\n",
        "\n",
        "#Removing the first 24 columns excluding the \"Adult\" and the \"Gender\" columns\n",
        "x_train=np.array([x_train1[:,15],x_train1[:,16],x_train1[:,17]]).T\n",
        "x_train=np.append(x_train.copy(),x_train1[:,24:],axis=1)\n",
        "x_test=np.array([x_test1[:,15],x_test1[:,16],x_test1[:,17]]).T\n",
        "x_test=np.append(x_test.copy(),x_test1[:,24:],axis=1)"
      ],
      "metadata": {
        "id": "c6EROTSp2h7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning"
      ],
      "metadata": {
        "id": "OEi0wH8gQe3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we compute the col_list of columns that contain at least 80% of information in x_train.\n",
        "#sortd1, sortd2 are the sorted versions of columns of x_train, x_test respectively.\n",
        "#sortd are used to remove outliers - everything under the 1-st and above the 99-th percentile.\n",
        "#After removing the outliers, we check if the column is constant, in which case we don't include it into col_list.\n",
        "#Otherwise, we substitute the nans by the mean of the known labels.\n",
        "#Finally, we subtract the columnwise mean and divide by columnwise std to normalize x_train and x_test.\n",
        "col_list=[]\n",
        "for i in range(x_train.shape[1]):\n",
        "  n=np.count_nonzero(np.isnan(x_train[:,i]))/x_train.shape[0]\n",
        "  if n<0.2:\n",
        "    sortd1=np.sort(x_train[np.argwhere(~np.isnan(x_train[:,i])),i].reshape(-1))\n",
        "    x_train[np.argwhere(x_train[:,i]>sortd1[int(0.99*len(sortd1))]),i]=np.nan\n",
        "    x_train[np.argwhere(x_train[:,i]<sortd1[int(0.01*len(sortd1))]),i]=np.nan\n",
        "    sortd2=np.sort(x_test[np.argwhere(~np.isnan(x_test[:,i])),i].reshape(-1))\n",
        "    x_test[np.argwhere(x_test[:,i]>sortd2[int(0.99*len(sortd2))]),i]=np.nan\n",
        "    x_test[np.argwhere(x_test[:,i]<sortd2[int(0.01*len(sortd2))]),i]=np.nan\n",
        "    if np.nanstd(x_train[:,i])==0:\n",
        "      pass\n",
        "    else:\n",
        "      x_train[:,i] = np.nan_to_num(x_train[:,i], nan=np.nanmean(x_train[:,i]))\n",
        "      x_test[:,i] = np.nan_to_num(x_test[:,i], nan=np.nanmean(x_test[:,i]))\n",
        "      col_list.append(i)\n",
        "  else:\n",
        "    pass\n",
        "x_train_mod=x_train[:,col_list]\n",
        "x_test_mod=x_test[:,col_list]\n",
        "# Feature Scaling (manually, if required)\n",
        "mean_x = np.mean(x_train_mod, axis=0)\n",
        "std_x = np.std(x_train_mod, axis=0)\n",
        "mean_test_x = np.mean(x_test_mod, axis=0)\n",
        "std_test_x = np.std(x_test_mod, axis=0)\n",
        "x_train_scaled = (x_train_mod - mean_x) / std_x\n",
        "tx_train = np.c_[np.ones((x_train_scaled.shape[0], 1)), x_train_scaled]\n",
        "x_test_scaled = (x_test_mod - mean_test_x) / std_test_x\n",
        "tx_test = np.c_[np.ones((x_test_scaled.shape[0], 1)), x_test_scaled]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "9TTtaiDU3J-q",
        "outputId": "e1a40ea4-d1e5-4402-9741-e10a6e619e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'gamma' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e59030506647>\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mx_test_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test_mod\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean_test_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd_test_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mtx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_scaled\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_scaled\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogistic_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_score_logistic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gamma' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "tzcmElhYQiK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different gammas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "gamma=[0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02]\n",
        "for i in range(11):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,gamma[i],0.02) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Hebm_gDl3mva",
        "outputId": "0ea6b3e9-7629-4f4d-d34d-eb7925257542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5948136142625607\n",
            "0.5702746365105008\n",
            "0.5611745513866232\n",
            "0.5814332247557004\n",
            "0 0.9128521448685285\n",
            "0.568259385665529\n",
            "0.5573248407643312\n",
            "0.6143790849673203\n",
            "0.5711920529801324\n",
            "1 0.9128490973144954\n",
            "0.594896331738437\n",
            "0.5607142857142857\n",
            "0.5721153846153846\n",
            "0.5763888888888888\n",
            "2 0.9128094791120647\n",
            "0.5525040387722132\n",
            "0.5788561525129983\n",
            "0.5694682675814752\n",
            "0.6111111111111112\n",
            "3 0.9127972888959321\n",
            "0.5182829888712241\n",
            "0.5730337078651685\n",
            "0.6087751371115173\n",
            "0.6037414965986394\n",
            "4 0.9127393853693025\n",
            "0.5722222222222222\n",
            "0.5825688073394495\n",
            "0.5547226386806596\n",
            "0.5339673913043478\n",
            "5 0.9127180524910706\n",
            "0.517931609674729\n",
            "0.528526148969889\n",
            "0.5321027287319422\n",
            "0.5276845637583892\n",
            "6 0.9124925334926188\n",
            "0.45659377070907886\n",
            "0.45\n",
            "0.4368421052631579\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0c724bd32701>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.002\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_logistic_regression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogistic_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_score_logistic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-23066660d067>\u001b[0m in \u001b[0;36mcross_val\u001b[0;34m(mach_learn, data_repr, score_func, tx, y, folds, max_iters, gamma, lambda_)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtest_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfold_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmach_learn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_w\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mpred_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-98ccf52df060>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[0;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m       \u001b[0mgrad\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "S8JeTatHqnWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different gammas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "gamma=[0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]\n",
        "for i in range(10):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,gamma[i],0.02) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "id": "yUwEC8wVqnFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "hglhopyGQmkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different lambdas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "lambda_=[0,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]\n",
        "for i in range(11):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,0.013,lambda_[i]) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "id": "_1BYiLfbPsHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results"
      ],
      "metadata": {
        "id": "lofTfBYSQq4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we plot the cross-validation score and the F1-score together in adjacent subplots for the experiments above\n",
        "import matplotlib.pyplot as plt\n",
        "cross_score_list=[[0.843634003181627, 0.8508261868626841, 0.8573509602784228, 0.8631290875069332, 0.8681605685482151, 0.8728568206891087, 0.8771325129367881, 0.8808566012665557, 0.8842149853413545, 0.8870095753564092, 0.889599980495773],\n",
        "                  [0.7494072543534044, 0.7620484314334998, 0.7977228815057263, 0.843634003181627, 0.889599980495773, 0.9124839242504587, 0.914044262404993, 0.9142911127770972, 0.9143124455253037, 0.9120725069636185],\n",
        "                  [0.8641286791371817, 0.8640799185698526, 0.8640250629316072, 0.8638757336941615, 0.8636166931802252, 0.8631290875069332, 0.861620557455186, 0.8593836664289589, 0.8542881871430574, 0.8390291771044756, 0.8174282457776396]]\n",
        "F1_score_list=[[0.39346228813457146, 0.39566160835252323, 0.39792074757988366, 0.39802134182007315, 0.39745122215939044, 0.39702857839874933, 0.39578429371577006, 0.39352459256389194, 0.3907926662959345, 0.3874109250728939, 0.3840502865308115],\n",
        "               [0.35098294372024075, 0.35762450313346944, 0.3749460774465487, 0.39346228813457146, 0.3840502865308115, 0.2682332940878564, 0.21131586043088726, 0.19528480572876492, 0.19208448488324978, 0.18962007390983215],\n",
        "               [0.39805835873418943, 0.39805405144621225, 0.39799031636393406, 0.3979562412143844, 0.3979542384452552, 0.39802134182007315, 0.39798104474211254, 0.3982652682394514, 0.3972691766847049, 0.3921244685464566, 0.38365032295726875]]\n",
        "varying_list=[[0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02],\n",
        "              [0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1],\n",
        "              [0,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]]\n",
        "title_list=['max_iters=100, lambda_=0.02, gamma varying', 'max_iters=100, lambda_=0.02, gamma varying', 'max_iters=100, gamma=0.013, lambda_ varying']\n",
        "x_label_list=['gamma','gamma','lambda_']\n",
        "for i in range(3):\n",
        "  fig,(ax1,ax2)=plt.subplots(2,1)\n",
        "  ax1.plot(varying_list[i],cross_score_list[i])\n",
        "  ax2.plot(varying_list[i],F1_score_list[i])\n",
        "  ax1.set_xlabel(x_label_list[i])\n",
        "  ax1.set_xscale('log')\n",
        "  ax1.set_ylabel('Cross-validation score')\n",
        "  ax1.set_title(title_list[i])\n",
        "  ax2.set_xlabel(x_label_list[i])\n",
        "  ax2.set_xscale('log')\n",
        "  ax2.set_ylabel('F1 score')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "9LjECKsEPy6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating CSV submission"
      ],
      "metadata": {
        "id": "lX2Fl4bBQnge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "from helpers import create_csv_submission\n",
        "w_pred=reg_logistic_regression((1+y_train)/2,tx_train,0.02,np.zeros(tx_train.shape[1]),100,0.013)[0] #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "y_pred=logistic_label(tx_test,w_pred)\n",
        "create_csv_submission(np.arange(328135,437514),2*y_pred-1,'submission.csv') #2*y_pred - 1 is used to rescale {0,1} to {-1,1}"
      ],
      "metadata": {
        "id": "TedawsHYPvqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}