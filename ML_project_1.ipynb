{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Base functions"
      ],
      "metadata": {
        "id": "ZTuboUA0bKt4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLGVFUWXEn_M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "#Notice that in this section, we have more parameters than needed, with the unneeded parameters pre-set to 0.\n",
        "#This is because we define a general function for the cross-validation.\n",
        "\n",
        "def mean_square_error_gd(y,tx,initial_w,max_iters,gamma,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  e=y-tx @ initial_w\n",
        "  for i in range(max_iters):\n",
        "    e=y-tx @ w\n",
        "    grad=- (tx.T @ e.copy()) / N\n",
        "    w-=gamma*grad\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def mean_square_error_sgd(y,tx,initial_w,max_iters,gamma,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via stochastic gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  coefs=np.random.choice(N,max_iters)\n",
        "  e=y-tx @ initial_w\n",
        "  for i in range(max_iters):\n",
        "    j=coefs[i]\n",
        "    e=y-tx @ w\n",
        "    stoch_grad=- (tx[j,:].T*e[j])\n",
        "    w-=gamma*stoch_grad\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def least_squares(y,tx,initial_w=0,max_iters=0,gamma=0,lambda_=0):\n",
        "  '''Computes the argmin/min of the quadratic function via normal equations of least squares problem.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=np.linalg.solve(tx.T @ tx, tx.T @ y)\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def ridge_regression(y,tx,lambda_,initial_w=0,max_iters=0,gamma=0):\n",
        "  '''Computes the argmin/min of the L2-regularized quadratic function via normal equations.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   lambda_ is a real number representing the regularization coefficient.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  M=tx.shape[1]\n",
        "  w=np.linalg.solve(tx.T @ tx + 2*N*lambda_*np.eye(M), tx.T @ y)\n",
        "  e=y-tx @ w\n",
        "  loss=np.linalg.norm(e)**2/(2*N)\n",
        "  return w,loss\n",
        "\n",
        "def sigmoid(x):\n",
        "  '''Computes the sigmoid function.\n",
        "\n",
        "  '''\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def logistic_regression(y, tx, initial_w, max_iters, gamma, lambda_=0):\n",
        "  '''Computes the argmin/min of the logistic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  for i in range(max_iters):\n",
        "    grad=0\n",
        "    for j in range(N):\n",
        "      grad+=(sigmoid(tx[j,:].T @ w)-y[j])*tx[j,:]\n",
        "    grad/=N\n",
        "    w-=gamma*grad\n",
        "  loss=0\n",
        "  for j in range(N):\n",
        "    w_pert=tx[j,:] @ w\n",
        "    loss+=-y[j]*(w_pert)+np.log(1+np.exp(w_pert))\n",
        "  loss/=N\n",
        "  return w,loss\n",
        "\n",
        "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
        "  '''Computes the argmin/min of the L2-regularized logistic function via gradient descent.\n",
        "   y is an (N,) array representing the vector of outputs.\n",
        "   tx is an (N,M) array representing the matrix of inputs.\n",
        "   lambda_ is a real number representing the regularization coefficient.\n",
        "   initial_w is an (M,) array representing the initial guess for the minimum.\n",
        "   max_iters is an integer representing the number of iterations.\n",
        "   gamma is a real number representing the step-size.\n",
        "   N represents the amount of data.\n",
        "   M represents the length of minimized vector.\n",
        "\n",
        "  '''\n",
        "  N=y.shape[0]\n",
        "  w=initial_w.copy()\n",
        "  for i in range(max_iters):\n",
        "    grad=0\n",
        "    for j in range(N):\n",
        "      grad+=(sigmoid(tx[j,:].T @ w)-y[j])*tx[j,:]\n",
        "    grad/=N\n",
        "    grad+=2*lambda_*w\n",
        "    w-=gamma*grad\n",
        "  loss=0\n",
        "  for j in range(N):\n",
        "    w_pert=tx[j,:] @ w\n",
        "    loss+=-y[j]*(w_pert)+np.log(1+np.exp(w_pert))\n",
        "  loss/=N\n",
        "  return w,loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross-validation function"
      ],
      "metadata": {
        "id": "f_e46ueSogSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def square_label(tx,w):\n",
        "  '''Returns the outputs based on the quadratic model.\n",
        "  tx is an (N,M) array representing the matrix of inputs.\n",
        "  w is an (M,) array representing the minimum of the quadratic function.\n",
        "\n",
        "  '''\n",
        "  return tx @ w\n",
        "\n",
        "def logistic_label(tx,w):\n",
        "  '''Returns the outputs based on the logistic model.\n",
        "  tx is an (N,M) array representing the matrix of inputs.\n",
        "  w is an (M,) array representing the minimum of the quadratic function.\n",
        "\n",
        "  '''\n",
        "  return np.round(sigmoid(tx @ w))\n",
        "\n",
        "def train_score_square(y_test, y_ML):\n",
        "  '''Computes the L2,L1 scores of the quadratic model.\n",
        "  y_test is an (N,) Boolean vector representing the true labels.\n",
        "  y_ML is an (N,) Boolean vector representing the predicted labels.\n",
        "\n",
        "  '''\n",
        "  N=y_test.shape[0]\n",
        "  return np.mean((y_test-y_ML)**2),np.mean(np.abs(y_test-y_ML))\n",
        "\n",
        "def train_score_logistic(y_test, y_ML):\n",
        "  '''Computes the probabilistic score and the F1 score of the logistic model.\n",
        "  y_test is an (N,) Boolean vector representing the true labels.\n",
        "  y_ML is an (N,) Boolean vector representing the predicted labels.\n",
        "\n",
        "  '''\n",
        "  N=y_test.shape[0]\n",
        "  P=len(np.argwhere(np.logical_and(y_ML==1,y_test==1)))/len(np.argwhere(y_test==1))\n",
        "  R=len(np.argwhere(np.logical_and(y_ML==1,y_test==1)))/len(np.argwhere(y_ML==1))\n",
        "  return np.mean(np.abs(y_test-y_ML)),2*P*R/(P+R)\n",
        "\n",
        "def cross_val(mach_learn,data_repr,score_func,tx,y,folds,max_iters=0,gamma=0,lambda_=0):\n",
        "  '''Cross-validation of the model. Returns two different model scores.\n",
        "  mach_learn - function representing the utilised method of machine learning.\n",
        "  data_repr - function representing the data.\n",
        "  score_func - function representing the score.\n",
        "  tx - (N,M) array representing the matrix of N input vectors.\n",
        "  y - (N,) array representing the vector of N outputs.\n",
        "  folds - integer representing the number of folds.\n",
        "  max_iters - integer representing the training time on each fold.\n",
        "  gamma is a real number representing the step-size.\n",
        "\n",
        "  '''\n",
        "  N,M=tx.shape\n",
        "  fold_size=N//folds\n",
        "  sf=np.random.permutation(N)\n",
        "  scores=np.ones(folds)\n",
        "  alt_scores=np.ones(folds)\n",
        "  for k in range(folds):\n",
        "    train_data=tx[np.append(sf[:k*fold_size],sf[(k+1)*fold_size:])].copy()\n",
        "    train_labels=y[np.append(sf[:k*fold_size],sf[(k+1)*fold_size:])].copy()\n",
        "    test_data=tx[sf[k*fold_size:(k+1)*fold_size]].copy()\n",
        "    test_labels=y[sf[k*fold_size:(k+1)*fold_size]].copy()\n",
        "    w=mach_learn(y=train_labels,tx=train_data,initial_w=np.zeros(M),max_iters=max_iters,gamma=gamma,lambda_=lambda_)[0]\n",
        "    pred_labels=data_repr(test_data,w)\n",
        "    scores[k],alt_scores[k]=score_func(test_labels,pred_labels)\n",
        "  return np.mean(scores),np.mean(alt_scores)"
      ],
      "metadata": {
        "id": "CNVKeqRafgQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading data"
      ],
      "metadata": {
        "id": "k50ogs6C16JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helpers import load_csv_data\n",
        "\n",
        "x_train, x_test, y_train,_,_= load_csv_data('/content/')"
      ],
      "metadata": {
        "id": "rvB5Y1y-15Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing the medically insignificant data (e.g. - telephone number)"
      ],
      "metadata": {
        "id": "1hyvwnJW2bgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train1=x_train.copy()\n",
        "x_test1=x_test.copy()\n",
        "\n",
        "#Removing the first 24 columns excluding the \"Adult\" and the \"Gender\" columns\n",
        "x_train=np.array([x_train1[:,15],x_train1[:,16],x_train1[:,17]]).T\n",
        "x_train=np.append(x_train.copy(),x_train1[:,24:],axis=1)\n",
        "x_test=np.array([x_test1[:,15],x_test1[:,16],x_test1[:,17]]).T\n",
        "x_test=np.append(x_test.copy(),x_test1[:,24:],axis=1)"
      ],
      "metadata": {
        "id": "c6EROTSp2h7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning"
      ],
      "metadata": {
        "id": "OEi0wH8gQe3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we compute the col_list of columns that contain at least 80% of information in x_train.\n",
        "#sortd1, sortd2 are the sorted versions of columns of x_train, x_test respectively.\n",
        "#sortd are used to remove outliers - everything under the 1-st and above the 99-th percentile.\n",
        "#After removing the outliers, we check if the column is constant, in which case we don't include it into col_list.\n",
        "#Otherwise, we substitute the nans by the mean of the known labels.\n",
        "#Finally, we subtract the columnwise mean and divide by columnwise std to normalize x_train and x_test.\n",
        "col_list=[]\n",
        "for i in range(x_train.shape[1]):\n",
        "  n=np.count_nonzero(np.isnan(x_train[:,i]))/x_train.shape[0]\n",
        "  if n<0.2:\n",
        "    sortd1=np.sort(x_train[np.argwhere(~np.isnan(x_train[:,i])),i].reshape(-1))\n",
        "    x_train[np.argwhere(x_train[:,i]>sortd1[int(0.99*len(sortd1))]),i]=np.nan\n",
        "    x_train[np.argwhere(x_train[:,i]<sortd1[int(0.01*len(sortd1))]),i]=np.nan\n",
        "    sortd2=np.sort(x_test[np.argwhere(~np.isnan(x_test[:,i])),i].reshape(-1))\n",
        "    x_test[np.argwhere(x_test[:,i]>sortd2[int(0.99*len(sortd2))]),i]=np.nan\n",
        "    x_test[np.argwhere(x_test[:,i]<sortd2[int(0.01*len(sortd2))]),i]=np.nan\n",
        "    if np.nanstd(x_train[:,i])==0:\n",
        "      pass\n",
        "    else:\n",
        "      x_train[:,i] = np.nan_to_num(x_train[:,i], nan=np.nanmean(x_train[:,i]))\n",
        "      x_test[:,i] = np.nan_to_num(x_test[:,i], nan=np.nanmean(x_test[:,i]))\n",
        "      col_list.append(i)\n",
        "  else:\n",
        "    pass\n",
        "x_train_mod=x_train[:,col_list]\n",
        "x_test_mod=x_test[:,col_list]\n",
        "\n",
        "mean_x = np.mean(x_train_mod, axis=0)\n",
        "std_x = np.std(x_train_mod, axis=0)\n",
        "mean_test_x = np.mean(x_test_mod, axis=0)\n",
        "std_test_x = np.std(x_test_mod, axis=0)\n",
        "x_train_scaled = (x_train_mod - mean_x) / std_x\n",
        "tx_train = np.c_[np.ones((x_train_scaled.shape[0], 1)), x_train_scaled]\n",
        "x_test_scaled = (x_test_mod - mean_test_x) / std_test_x\n",
        "tx_test = np.c_[np.ones((x_test_scaled.shape[0], 1)), x_test_scaled]"
      ],
      "metadata": {
        "id": "9TTtaiDU3J-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "tzcmElhYQiK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different gammas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "gamma=[0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02]\n",
        "for i in range(11):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,gamma[i],0.02) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "id": "Hebm_gDl3mva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "S8JeTatHqnWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different gammas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "gamma=[0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]\n",
        "for i in range(10):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,gamma[i],0.02) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "id": "yUwEC8wVqnFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing different parameters"
      ],
      "metadata": {
        "id": "hglhopyGQmkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we are testing the cross-validation and F1 scores for different lambdas.\n",
        "cross_score=list()\n",
        "F1_score=list()\n",
        "lambda_=[0,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]\n",
        "for i in range(11):\n",
        "  np.random.seed(0)\n",
        "  c,f1=cross_val(reg_logistic_regression,logistic_label,train_score_logistic,tx_train,(1+y_train)/2,3,100,0.013,lambda_[i]) #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "  cross_score.append(1-c) #c represents a probability of failure, to compute the score we compute 1-c\n",
        "  F1_score.append(f1) #f1 represents the F1-score\n",
        "print(cross_score, '\\n', F1_score)"
      ],
      "metadata": {
        "id": "_1BYiLfbPsHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the results"
      ],
      "metadata": {
        "id": "lofTfBYSQq4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here, we plot the cross-validation score and the F1-score together in adjacent subplots for the experiments above\n",
        "import matplotlib.pyplot as plt\n",
        "cross_score_list=[[0.843634003181627, 0.8508261868626841, 0.8573509602784228, 0.8631290875069332, 0.8681605685482151, 0.8728568206891087, 0.8771325129367881, 0.8808566012665557, 0.8842149853413545, 0.8870095753564092, 0.889599980495773],\n",
        "                  [0.7494072543534044, 0.7620484314334998, 0.7977228815057263, 0.843634003181627, 0.889599980495773, 0.9124839242504587, 0.914044262404993, 0.9142911127770972, 0.9143124455253037, 0.9120725069636185],\n",
        "                  [0.8641286791371817, 0.8640799185698526, 0.8640250629316072, 0.8638757336941615, 0.8636166931802252, 0.8631290875069332, 0.861620557455186, 0.8593836664289589, 0.8542881871430574, 0.8390291771044756, 0.8174282457776396]]\n",
        "F1_score_list=[[0.39346228813457146, 0.39566160835252323, 0.39792074757988366, 0.39802134182007315, 0.39745122215939044, 0.39702857839874933, 0.39578429371577006, 0.39352459256389194, 0.3907926662959345, 0.3874109250728939, 0.3840502865308115],\n",
        "               [0.35098294372024075, 0.35762450313346944, 0.3749460774465487, 0.39346228813457146, 0.3840502865308115, 0.2682332940878564, 0.21131586043088726, 0.19528480572876492, 0.19208448488324978, 0.18962007390983215],\n",
        "               [0.39805835873418943, 0.39805405144621225, 0.39799031636393406, 0.3979562412143844, 0.3979542384452552, 0.39802134182007315, 0.39798104474211254, 0.3982652682394514, 0.3972691766847049, 0.3921244685464566, 0.38365032295726875]]\n",
        "varying_list=[[0.01,0.011,0.012,0.013,0.014,0.015,0.016,0.017,0.018,0.019,0.02],\n",
        "              [0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1],\n",
        "              [0,0.001,0.002,0.005,0.01,0.02,0.05,0.1,0.2,0.5,1]]\n",
        "title_list=['max_iters=100, lambda_=0.02, gamma varying', 'max_iters=100, lambda_=0.02, gamma varying', 'max_iters=100, gamma=0.013, lambda_ varying']\n",
        "x_label_list=['gamma','gamma','lambda_']\n",
        "for i in range(3):\n",
        "  fig,(ax1,ax2)=plt.subplots(2,1)\n",
        "  ax1.plot(varying_list[i],cross_score_list[i])\n",
        "  ax2.plot(varying_list[i],F1_score_list[i])\n",
        "  ax1.set_xlabel(x_label_list[i])\n",
        "  ax1.set_xscale('log')\n",
        "  ax1.set_ylabel('Cross-validation score')\n",
        "  ax1.set_title(title_list[i])\n",
        "  ax2.set_xlabel(x_label_list[i])\n",
        "  ax2.set_xscale('log')\n",
        "  ax2.set_ylabel('F1 score')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "9LjECKsEPy6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating CSV submission"
      ],
      "metadata": {
        "id": "lX2Fl4bBQnge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "from helpers import create_csv_submission\n",
        "w_pred=reg_logistic_regression((1+y_train)/2,tx_train,0.02,np.zeros(tx_train.shape[1]),100,0.013)[0] #(1+y_train)/2 is used to rescale {-1,1} to {0,1}\n",
        "y_pred=logistic_label(tx_test,w_pred)\n",
        "create_csv_submission(np.arange(328135,437514),2*y_pred-1,'submission.csv') #2*y_pred - 1 is used to rescale {0,1} to {-1,1}"
      ],
      "metadata": {
        "id": "TedawsHYPvqs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}